{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20639b4c",
   "metadata": {},
   "source": [
    "transformer出发，里面的attention、fnn、pe、layernorm、relu函数，各自都有改进。\n",
    "\n",
    "学习目标：\n",
    "\n",
    "1. transformer相关\n",
    "- pe\n",
    "- attn: se\\mha\n",
    "- fnn\n",
    "- norm:layernorm\\batchnorm\n",
    "- Relu\n",
    "- decoder only\n",
    "\n",
    "2. llama/其他改进\n",
    "- atte:GQA/MQA/flashAtten\n",
    "- deepseek：MoE\n",
    "- RoPE/YaRN\n",
    "- SwiGLU\n",
    "\n",
    "3. minimind构建post train\n",
    "- LoRA\n",
    "- RL:DPO/PPO\n",
    "- vLLM：kv cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72eaac",
   "metadata": {},
   "source": [
    "# 0. embedding与位置编码相关\n",
    "- emd【tokenizer部分写】\n",
    "$$\\text{Input} \\xrightarrow{\\text{分词}} \\text{Token} \\xrightarrow{\\text{Vocabulary}} \\text{Token ID} \\xrightarrow{\\text{Embedding}} \\mathbf{X}_{(\\text{emd})} \\xrightarrow{\\oplus \\text{PE}} \\mathbf{X}_{\\text{final}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b7444c",
   "metadata": {},
   "source": [
    "- PE $seqlen \\times d_{model}$\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81897dcd",
   "metadata": {},
   "source": [
    "div处理\n",
    "\n",
    "$$\\text{before} = \\frac{1}{10000^{2i / d_{model}}}$$\n",
    "\n",
    "$$\\text{after} = e^{\\left( - \\frac{2i}{d_{model}} \\times \\ln(10000) \\right)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b24f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "tensor([1.0000e+00, 8.3176e-01, 6.9183e-01, 5.7544e-01, 4.7863e-01, 3.9811e-01,\n",
      "        3.3113e-01, 2.7542e-01, 2.2909e-01, 1.9055e-01, 1.5849e-01, 1.3183e-01,\n",
      "        1.0965e-01, 9.1201e-02, 7.5858e-02, 6.3096e-02, 5.2481e-02, 4.3652e-02,\n",
      "        3.6308e-02, 3.0200e-02, 2.5119e-02, 2.0893e-02, 1.7378e-02, 1.4454e-02,\n",
      "        1.2023e-02, 1.0000e-02, 8.3176e-03, 6.9183e-03, 5.7544e-03, 4.7863e-03,\n",
      "        3.9811e-03, 3.3113e-03, 2.7542e-03, 2.2909e-03, 1.9055e-03, 1.5849e-03,\n",
      "        1.3183e-03, 1.0965e-03, 9.1201e-04, 7.5858e-04, 6.3096e-04, 5.2481e-04,\n",
      "        4.3652e-04, 3.6308e-04, 3.0200e-04, 2.5119e-04, 2.0893e-04, 1.7378e-04,\n",
      "        1.4454e-04, 1.2023e-04])\n",
      "输入 x 的 Shape:   torch.Size([1, 3, 4])\n",
      "------------------------------\n",
      "切出来的 PE Shape:  torch.Size([1, 3, 4])\n",
      "------------------------------\n",
      "偶数位 (0::2): [0, 2, 4, 6, 8]\n",
      "奇数位 (1::2): [1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "d_model = 100\n",
    "max_len = 5000\n",
    "\n",
    "# i 取range\n",
    "\n",
    "i = torch.arange(0,d_model,2) # [s,e,step]\n",
    " \n",
    "div = torch.exp(i.float()*(-1)*(math.log(10000)/d_model)) # log底数默认为e，math.log(x, [base])\n",
    "\n",
    "print(i.shape)\n",
    "print(div)\n",
    "\n",
    "\n",
    "# input理解\n",
    "d_model = 4\n",
    "seq_len = 3  # 比如句子是 \"I love you\"\n",
    "batch_size = 1\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "print(f\"输入 x 的 Shape:   {x.shape}\")\n",
    "\n",
    "pe = torch.zeros(1, max_len, d_model)\n",
    "\n",
    "# 4. 执行切片\n",
    "sliced_pe = pe[:, :x.size(1), :]\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"切出来的 PE Shape:  {sliced_pe.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 奇偶理解\n",
    "\n",
    "indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# 1. 取偶数位 (0::2)\n",
    "evens = indices[0::2]\n",
    "print(f\"偶数位 (0::2): {evens}\")\n",
    "# 输出: [0, 2, 4, 6, 8]\n",
    "\n",
    "# 2. 取奇数位 (1::2)\n",
    "odds = indices[1::2]\n",
    "print(f\"奇数位 (1::2): {odds}\")\n",
    "# 输出: [1, 3, 5, 7, 9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8dd757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([4, 64, 512])\n",
      "输出形状: torch.Size([4, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PE(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model ,max_len = 5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. pe init [max_len,d_model]\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 2. pos [maxlen,1] 【后续YaRN改进motivation，超过max_len未见过】\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)  \n",
    "    \n",
    "        # 3. div【token内部d_model做高低频】\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # [d_model/2]\n",
    "        \n",
    "        # 4. mul,对d_model维度分奇偶处理\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) \n",
    "        pe[:, 1::2] = torch.cos(position * div_term) \n",
    "        \n",
    "        \n",
    "        self.register_buffer('pe',pe.unsqueeze(0)) # [batch,seq_len,hidden_dim]\n",
    "        \n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        #  x shape [batch,seq_len,hidden_dim]\n",
    "    \n",
    "        # x = x + position\n",
    "        \n",
    "        return x + self.pe[: , :x.size(1) , :] # 对做好的buffer pe 拿出seq_len\n",
    "\n",
    "d_model = 512\n",
    "max_len = 5000\n",
    "seq_len = 64\n",
    "batch_size = 4\n",
    "\n",
    "# 初始化模型\n",
    "pe_layer = PE(d_model=d_model, max_len=max_len)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "output = pe_layer(x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")          # [4, 64, 512]\n",
    "print(f\"输出形状: {output.shape}\")     # [4, 64, 512]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd2c5a",
   "metadata": {},
   "source": [
    "- RoPE\n",
    "\n",
    "\n",
    "(1)freq:\n",
    "$$\\Theta_j = \\frac{1}{10000^{\\frac{2j}{d}}} = 10000^{-\\frac{2j}{d}}$$\n",
    "\n",
    "旋转变化公式：\n",
    "$$f(x, m) = \\begin{pmatrix} \\cos m\\theta & -\\sin m\\theta \\\\ \\sin m\\theta & \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n",
    "\n",
    "展开：\n",
    "$$\\begin{pmatrix} x_1 \\cos m\\theta - x_2 \\sin m\\theta \\\\ x_1 \\sin m\\theta + x_2 \\cos m\\theta \\end{pmatrix}$$\n",
    "\n",
    "拆开:\n",
    "$$\\begin{pmatrix} x_1' \\\\ x_2' \\end{pmatrix} = \\underbrace{\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\cdot \\cos \\theta}_{\\text{第一部分}} + \\underbrace{\\begin{pmatrix} -x_2 \\\\ x_1 \\end{pmatrix} \\cdot \\sin \\theta}_{\\text{第二部分}}$$\n",
    "\n",
    "(2)因此需要rotate_half：\n",
    "\n",
    "$$\\begin{pmatrix} -x_2 \\\\ x_1 \\end{pmatrix}$$\n",
    "\n",
    "(3)应用公式：\n",
    "$$\\text{RoPE}(x) = x \\otimes \\cos(m\\theta) + \\text{Rotate}(x) \\otimes \\sin(m\\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a20b50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 10, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# 1. freqs： cos 和 sin 预计算\n",
    "# 2. apply_rope\n",
    "\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "   \n",
    "    # 1. theta_i = 10000^(-2i/d)\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    \n",
    "    # 2. [0, 1, ..., max_len]\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    \n",
    "    # 3.类似register_buffer  : (end, dim/2)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    \n",
    "    # 4. cos 和 sin\n",
    "    freqs_cos = torch.cos(freqs)  # shape: (end, dim/2)\n",
    "    freqs_sin = torch.sin(freqs)  # shape: (end, dim/2)\n",
    "    \n",
    "    return freqs_cos, freqs_sin\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    \n",
    "    # freqs (max_len, dim/2) -> (1, max_len, 1, dim/2)\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "\n",
    "    \n",
    "    # 1. 旋转矩阵展开[x1,x2]与[-x2,x1]\n",
    "    def rotate_half(x):\n",
    "        x1 = x[..., : x.shape[-1] // 2] \n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        return torch.cat((-x2, x1), dim=-1)\n",
    "    \n",
    "    # 2. reshape 以便广播\n",
    "\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq)\n",
    "\n",
    "    # 3. \n",
    "    # rotate_half 计算\n",
    "    # x_rotated = x * cos + rotate_half(x) * sin\n",
    "    xq_out = (xq * freqs_cos) + (rotate_half(xq) * freqs_sin)\n",
    "    xk_out = (xk * freqs_cos) + (rotate_half(xk) * freqs_sin)\n",
    "    \n",
    "    return xq_out, xk_out\n",
    "\n",
    "\n",
    "dim = 64\n",
    "seq_len = 10\n",
    "bsz = 2\n",
    "num_heads = 4\n",
    "\n",
    "# 1. register_buffer  取出sin cos\n",
    "cos, sin = precompute_freqs_cis(dim, seq_len) # (end, dim/2)\n",
    "\n",
    "\n",
    "cos = torch.cat([cos, cos], dim=-1)\n",
    "sin = torch.cat([sin, sin], dim=-1)\n",
    "\n",
    "# input\n",
    "xq = torch.randn(bsz, seq_len, num_heads, dim)\n",
    "xk = torch.randn(bsz, seq_len, num_heads, dim)\n",
    "\n",
    "# RoPE\n",
    "xq_rope, xk_rope = apply_rotary_emb(xq, xk, cos, sin)\n",
    "\n",
    "print(\"Output shape:\", xq_rope.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ba845",
   "metadata": {},
   "source": [
    "# 1.Attention\n",
    "\n",
    "- self attention\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "- Tensor.masked_fill(mask, value) 。其中mask需要为bool\n",
    "-  torch.where(条件，mask==True 赋什么值，不满足条件的值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51ff414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1165,  1.4655, -1.3211,  1.2021],\n",
      "        [-2.6029, -0.5984, -0.2838, -0.5854],\n",
      "        [-0.6428, -1.1741, -1.9348, -0.2217],\n",
      "        [-0.5321, -1.2354,  1.3057,  1.7891]])\n",
      "tensor([[False,  True,  True,  True],\n",
      "        [False, False,  True,  True],\n",
      "        [False, False, False,  True],\n",
      "        [False, False, False, False]])\n",
      "tensor([[-1.1165,  0.0000,  0.0000,  0.0000],\n",
      "        [-2.6029, -0.5984,  0.0000,  0.0000],\n",
      "        [-0.6428, -1.1741, -1.9348,  0.0000],\n",
      "        [-0.5321, -1.2354,  1.3057,  1.7891]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[ 9.0990e-01, -1.0000e+20, -1.0000e+20, -1.0000e+20],\n",
      "        [-1.1419e+00, -2.1434e+00, -1.0000e+20, -1.0000e+20],\n",
      "        [-1.6208e-01,  1.6470e+00, -1.4175e+00, -1.0000e+20],\n",
      "        [-7.9996e-01,  1.0003e+00, -1.4757e+00,  3.1392e-01]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "scores = torch.randn(4, 4)\n",
    "print(scores)\n",
    "\n",
    "causal_mask = torch.ones(4, 4).triu(diagonal=1).bool()\n",
    "# mask_1 = torch.ones(4, 4).tril(0).bool() # 主对角线以及以下【需要做成bool形式】\n",
    "# mask_2 = torch.ones(4, 4).tril(-1) # 主对角线以下\n",
    "# mask_3 = torch.ones(4, 4).tril(2) # 主对角线以上2行\n",
    "print(causal_mask)\n",
    "# print(mask_2) \n",
    "# print(mask_3)\n",
    "\n",
    "# masked_fill(mask, value) \n",
    "scores  = scores.masked_fill(mask= causal_mask , value = 0)\n",
    "print(scores)\n",
    "\n",
    "# scores_where = torch.where(causal_mask, torch.tensor(0.0), scores) # True 位置设置为0\n",
    "# print( scores_where)\n",
    "\n",
    "\n",
    "# 这个方法比较正常\n",
    "scores2 = torch.randn(4, 4)\n",
    "att_weight = torch.ones(4, 4).tril() # 下三角矩阵 包含主对角线【default diagonal=0】\n",
    "print(att_weight)\n",
    "att_weight = scores2.masked_fill(att_weight == 0, float(\"-1e20\"))\n",
    "\n",
    "print(att_weight)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7abc496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 4])\n",
      "tensor([[[-0.8004, -0.6087],\n",
      "         [-0.8004, -0.6087],\n",
      "         [-0.8004, -0.6087],\n",
      "         [-0.8004, -0.6087]],\n",
      "\n",
      "        [[-0.5196, -0.4982],\n",
      "         [-0.6512, -0.6370],\n",
      "         [-0.6499, -0.6350],\n",
      "         [-0.6488, -0.6331]],\n",
      "\n",
      "        [[-0.2124, -0.2026],\n",
      "         [-0.3469, -0.3512],\n",
      "         [-0.3760, -0.3457],\n",
      "         [-0.2301, -0.1825]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "\n",
    "class SeAtten(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        # q/k/v/o \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1) # 一般设置为 0.1\n",
    "        \n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "    \n",
    "    def forward(self, x, atten_mask = None):\n",
    "        # input x: [batch,seq_len,hidden_dim]\n",
    "        \n",
    "        \n",
    "        # q/k/v projection \n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        \n",
    "        # attention value \n",
    "        ## dot : @ or torch.matmul\n",
    "        # q [batch,seq_len,hidden_dim]\n",
    "        # k [batch,seq_len,hidden_dim] =>【transpose】 [batch,hidden_dim,seq_len]\n",
    "        \n",
    "        # attention weight \n",
    "        ## scale : math.sqrt(hidden_dim)\n",
    "\n",
    "        weight = (q @ k.transpose(-2,-1)) / math.sqrt(self.hidden_dim) # [batch,seq_len_q,seq_len_k]\n",
    "        \n",
    "        # attention mask\n",
    "        if atten_mask is not None:\n",
    "            # weight  = weight.where(atten_mask,float('-inf'),weight) # (mask, masked_value【mask==True】, score【mask==False】)\n",
    "            weight = weight.masked_fill(atten_mask, float('-inf')) # (mask, masked_value【mask==True】)\n",
    "        \n",
    "        # apply softmax\n",
    "        weight = torch.softmax(weight, dim=-1)  #[batch,seq_len_q,seq_len_k] 对k维度【dim = -1】做softmax\n",
    "        \n",
    "        # apply dropout\n",
    "        weight = self.dropout(weight)\n",
    "\n",
    "        # out projection\n",
    "        \n",
    "        # [batch,seq_len,seq_len] @ [batch,seq_len,hidden_dim] => [batch,seq_len,hidden_dim]\n",
    "        out  = weight.matmul(v) # [batch,seq_len,hidden_dim]\n",
    "        \n",
    "        final = self.o_proj(out)\n",
    "           \n",
    "\n",
    "    \n",
    "        return final\n",
    "        \n",
    "        \n",
    "# test\n",
    "## randn生成随机数张量（Tensor）\n",
    "x = torch.randn(3, 4, 2) # [batch,seq_len,hidden_dim]\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "print(b.shape)\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1).bool()  #  * time\n",
    "print(mask.shape) # [3, 4, 4] 与atten value一致\n",
    "\n",
    "net  = SeAtten(hidden_dim=2) # hidden_dim保持一致\n",
    "\n",
    "\n",
    "output  = net(x,atten_mask = mask)\n",
    "print(output)\n",
    "print(output.shape) # [3, 4, 2]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f5411",
   "metadata": {},
   "source": [
    "- Multi-Head Attention\n",
    "\n",
    "$$\\text{Head}_i = \\text{Attention}(\\mathbf{Q} \\mathbf{W}_i^Q, \\mathbf{K} \\mathbf{W}_i^K, \\mathbf{V} \\mathbf{W}_i^V)$$\n",
    "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Softmax}\\left(\\frac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right) \\mathbf{V}$$\n",
    "$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{Head}_1, \\dots, \\text{Head}_h) \\mathbf{W}^O$$\n",
    "- view()/contiguous()做连续内存的拆分与合并,premute()/transpose()做交换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c7ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 2, 2])\n",
      "torch.Size([2, 2, 3, 2])\n",
      "torch.Size([2, 2, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "test  = torch.randn(2,3,4) # [b,s,hidden_dim]\n",
    "\n",
    "batch_size, seq_len, _ = test.size()\n",
    "print(batch_size, seq_len)\n",
    "print(test.shape) # [2, 3, 4]\n",
    "\n",
    "\n",
    "heads_num = 2\n",
    "head_dim = 4 // heads_num\n",
    "\n",
    "test = test.view(batch_size, seq_len, heads_num, head_dim)\n",
    "print(test.shape) # [2, 3, 2, 2]\n",
    "\n",
    "test1 = test\n",
    "# permute 位置idx写在当前位置\n",
    "test = test.permute(0,2,1,3)  # [b,heads,s,head_dim]\n",
    "test1 = test1.transpose(2,1) # [2,1] [1,2]都都行\n",
    "\n",
    "print(test.shape) # [2, 2, 3, 2]\n",
    "print(test1.shape) # [2, 2, 3, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c985e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head):\n",
    "        super().__init__()\n",
    "        self.nums_head = nums_head\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.head_dim = hidden_dim // nums_head # MHA 计算才用\n",
    "        \n",
    "        # qkvo\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, atten_mask = None):\n",
    "        # input x: [batch,seq_len,hidden_dim]\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        \n",
    "        # qkv projection [batch,seq_len,hidden_dim]\n",
    "        q = self.q_proj(x) \n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        \n",
    "        # MHA x: \n",
    "        ## [batch,seq_len,hidden_dim]=>【view】[batch,seq_len,nums_head,head_dim]\n",
    "        ## [batch,seq_len,nums_head,head_dim]=>【transpose/permute】 [batch,nums_head,seq_len,head_dim]\n",
    "        \n",
    "        # q_s  = q.view(batch, seq_len, self.nums_head, self.head_dim).permute(0, 2, 1, 3)\n",
    "        q_s = q.view(batch, seq_len, self.nums_head, self.head_dim).transpose(1,2)\n",
    "        k_s = k.view(batch, seq_len, self.nums_head, self.head_dim).transpose(1,2)\n",
    "        v_s = v.view(batch, seq_len, self.nums_head, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        # attention value ：q*k.T / sqrt(head_dim)\n",
    "        # k_s [batch,nums_head,seq_len,head_dim]=> [batch,nums_head,head_dim,seq_len]\n",
    "        \n",
    "        weight = q_s @ k_s.transpose(-2,-1) / math.sqrt(self.head_dim) # [batch,nums_head,seq_len,seq_len]\n",
    "        \n",
    "        # attention mask\n",
    "        if atten_mask is not None:\n",
    "            weight = weight.masked_fill(atten_mask==0, float('-inf'))\n",
    "            \n",
    "        # softmax key dim\n",
    "        weight = torch.softmax(weight, dim = -1) \n",
    "        \n",
    "        # apply dropout\n",
    "        weight = self.dropout(weight)\n",
    "        \n",
    "        # *v and concat\n",
    "        ## [b,nums_h,s,s] @ [b,nums_h,s,d]=>[b,nums_h,s,d]\n",
    "        out = weight @ v_s  # [b,nums_h,s,d]\n",
    "        ## concat  [batch,nums_h,seq_len,head_dim]=>   [batch,seq_len,hidden_dim]\n",
    "        out = out.transpose(1,2).contiguous().view(batch, seq_len, -1)\n",
    "        \n",
    "        \n",
    "        final = self.o_proj(out)\n",
    "        \n",
    "        return final\n",
    "    \n",
    "# test\n",
    "\n",
    "attention_mask = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [0, 1],\n",
    "            [0, 0],\n",
    "            [1, 0],\n",
    "        ]\n",
    "    )\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .expand(3, 8, 2, 2)\n",
    ")\n",
    "\n",
    "x = torch.rand(3, 2, 128)\n",
    "net = MHA(128, 8)\n",
    "net(x, attention_mask).shape\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f4322",
   "metadata": {},
   "source": [
    "- GQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4197318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 8, 128])\n",
      "torch.Size([3, 16, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "# [batch,nums_kv_group,seq_len,head_dim]\n",
    "k = torch.rand(3, 2, 8, 128)\n",
    "nums_head = 16\n",
    "nums_key_value_head = 2\n",
    "\n",
    "print(k.shape) # [3, 2, 8, 128]\n",
    "# repeat nums_kv_group => nums_head 次数\n",
    "k = k.repeat_interleave(nums_head// nums_key_value_head, dim=1) # [3, 16, 8, 128]\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42128bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class GQA(nn.Module):\n",
    "    def __init__(self,hidden_dim,nums_head,nums_kv_group):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Q/G judge\n",
    "        assert hidden_dim % nums_head ==0\n",
    "        assert nums_head % nums_kv_group == 0\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.nums_head = nums_head\n",
    "\n",
    "        # Q/G init\n",
    "        self.head_dim = hidden_dim // nums_head # 计算kv dim\n",
    "        self.nums_kv_group = nums_kv_group\n",
    "        \n",
    "        # qkvo【nums_head与nums_kv_group的区别】\n",
    "        \n",
    "        # q shape  (nums_head * head_dim)\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim) \n",
    "        # kv shape (nums_key_value_head * head_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,nums_kv_group*self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,nums_kv_group*self.head_dim)\n",
    "        \n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        # input x: [batch,seq_len,hidden_dim]\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        # qkv projection\n",
    "        q = self.q_proj(x) # [batch,seq_len,hidden_dim]\n",
    "        k = self.k_proj(x) # [batch,seq_len,nums_kv_group*self.head_dim]\n",
    "        v = self.v_proj(x)     \n",
    "        \n",
    "        # mha split\n",
    "        \n",
    "        # q [batch,seq_len,hidden_dim]=>[batch,nums_head,seq_len,head_dim]\n",
    "        q =  q.view(batch, seq_len, self.nums_head, self.head_dim).transpose(1,2)\n",
    "        # kv [batch,seq_len,hidden_dim]=>[batch,nums_kv_group,seq_len,head_dim]\n",
    "        k =  k.view(batch, seq_len, self.nums_kv_group, self.head_dim).transpose(1,2)\n",
    "        v =  v.view(batch, seq_len, self.nums_kv_group, self.head_dim).transpose(1,2)\n",
    "                      \n",
    "        # kv repeat nums_head//nums_kv_group times dim = 1\n",
    "        #  [batch,nums_kv_group,seq_len,head_dim] => [batch,nums_head,seq_len,head_dim]\n",
    "        k = k.repeat_interleave(self.nums_head // self.nums_kv_group, dim = 1)\n",
    "        v = v.repeat_interleave(self.nums_head // self.nums_kv_group, dim = 1)\n",
    "        \n",
    "        # weight\n",
    "        weight = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim) # [batch,nums_head,seq_len,seq_len]\n",
    "        weight = torch.softmax(weight, dim = -1)\n",
    "        \n",
    "        # @v  \n",
    "        #  [batch,nums_head,seq_len,seq_len] @ [batch,nums_head,seq_len,head_dim] => [batch,nums_head,seq_len,head_dim]\n",
    "        out  = weight @ v #  [batch,nums_head,seq_len,head_dim]\n",
    "        \n",
    "        # concat\n",
    "        # [batch,nums_head,seq_len,head_dim] => [batch,seq_len,nums_head,head_dim]  => [batch,seq_len,hidden_dim] \n",
    "        \n",
    "        out = out.transpose(1,2).contiguous().view(batch, seq_len, -1)\n",
    "        \n",
    "        # o proj\n",
    "        final = self.o_proj(out)\n",
    "        \n",
    "        return final\n",
    "    \n",
    "    \n",
    "# test\n",
    "x = torch.rand(3, 2, 128)\n",
    "net = GQA(hidden_dim=128, nums_head=16, nums_kv_group=2)\n",
    "out = net(x)\n",
    "print(out.shape) # [3, 2, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33782feb",
   "metadata": {},
   "source": [
    "- MQA : GQA的nums_kv_group ==1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7587e2c",
   "metadata": {},
   "source": [
    "# 2.Norm相关\n",
    "\n",
    "- layernorm\n",
    "\n",
    "均值 ($\\mu$)：$$\\mu = \\frac{1}{\\mathbf{H}} \\sum_{i=1}^{\\mathbf{H}} x_i$$  \n",
    "方差 ($\\sigma^2$)：$$\\sigma^2 = \\frac{1}{\\mathbf{H}} \\sum_{i=1}^{\\mathbf{H}} (x_i - \\mu)^2$$\n",
    "公式：\n",
    "$$\\mathbf{y} = \\frac{\\mathbf{x} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\cdot \\gamma + \\beta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425d505",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def forward():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d13c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 4])\n",
      "Output shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, h:int, eps:float=1e-8):\n",
    "        super().__init__()\n",
    "        self.h  = h                  # normalized_shape 通常是 [hidden_dim]\n",
    "        self.gamma = nn.Parameter(torch.ones(self.h))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.h))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def _norm(self,x):\n",
    "        # 1. mean\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        # 2. variance\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        \n",
    "        # 3. norm item\n",
    "        return (x - mean) * torch.rsqrt(var + self.eps)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x [batch, seq_len, hidden_dim]\n",
    "     \n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        \n",
    "        return self.gamma * out + self.beta\n",
    "\n",
    "# 测试代码\n",
    "x = torch.randn(2, 3, 4)  # [batch, seq_len, hidden_dim]\n",
    "ln = LayerNorm(4)\n",
    "out = ln(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3874b4b",
   "metadata": {},
   "source": [
    "- RMSNorm\n",
    "\n",
    "均方根 ($\\text{RMS}$)：\n",
    "    \n",
    "$$\\text{RMS}(\\mathbf{x}) = \\sqrt{\\frac{1}{\\mathbf{H}} \\sum_{i=1}^{\\mathbf{H}} x_i^2}$$\n",
    "\n",
    "公式\n",
    "\n",
    "$$y_{i}=\\frac{x_{i}}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}\\right)^{2}+\\epsilon}} * \\gamma$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe9f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "16\n",
      "tensor(4.)\n",
      "tensor(0.5000)\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]]) torch.Size([3, 1])\n",
      "tensor([1., 1., 1.]) torch.Size([3])\n",
      "tensor([[1., 1., 1., 1.]]) torch.Size([1, 4])\n",
      "tensor(1.) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 平方\n",
    "a = 2**2\n",
    "print(a)\n",
    "a2 = pow(a,2)\n",
    "print(a2)\n",
    "\n",
    "# 开方求倒数\n",
    "t = torch.tensor(4.0)\n",
    "print(t)\n",
    "a  =  torch.rsqrt(t)\n",
    "print(a)\n",
    "\n",
    "\n",
    "# mean\n",
    "t2  = torch.ones(3,4)\n",
    "mean_val1 = t2.mean(dim=1, keepdim=True) # 按列方向求平均,保持列结构\n",
    "print(mean_val1, mean_val1.shape) # [3,1]\n",
    "t2  = torch.ones(3,4)\n",
    "mean_val2 = t2.mean(dim=1, keepdim=False) # 按列方向求平均，无列结构\n",
    "print(mean_val2, mean_val2.shape) # [3,]\n",
    "\n",
    "\n",
    "t2  = torch.ones(3,4)\n",
    "mean_val1 = t2.mean(dim=0, keepdim=True) # 按行方向求平均，保持行结构\n",
    "print(mean_val1, mean_val1.shape) # [1,4]\n",
    "\n",
    "t2  = torch.ones(3,4)\n",
    "mean_val1 = t2.mean() # 所有元素求平均\n",
    "print(mean_val1, mean_val1.shape) # 标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972326d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4]) torch.Size([4])\n",
      "torch.Size([4, 1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# * and @\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "X = torch.tensor([1.0, 2.0, 3.0, 4.0]) \n",
    "print(X.shape)\n",
    "gamma = torch.tensor([10.0, 1.0, 2.0, 0.5]) \n",
    "\n",
    "# [1,4] * [1,4] => [1,4] 逐元素乘法\n",
    "Y_element_wise = X * gamma\n",
    "print(gamma.shape,Y_element_wise.shape)\n",
    "\n",
    "# [1,4] @ [4,1] => [1,1]\n",
    "gamma_matrix = gamma.view(4, 1) \n",
    "Z_matrix = X @ gamma_matrix\n",
    "print(gamma_matrix.shape, Z_matrix.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29486f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.6832, -0.5923,  0.8886,  0.1621],\n",
      "         [-0.5696, -0.1878, -1.3380,  1.3601],\n",
      "         [ 0.5076, -0.2491,  1.2554,  1.4506]],\n",
      "\n",
      "        [[ 1.3700,  0.8199,  0.1846,  1.1903],\n",
      "         [-0.2055,  0.5257, -0.6524, -1.8044],\n",
      "         [-0.6674,  0.2872, -1.3462,  1.2883]]], grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self,h:int,eps:float=1e-8):\n",
    "        super().__init__()\n",
    "        self.h = h\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(h))# learnable parameter,对每个token的每个hidden_dim进行scale\n",
    "        \n",
    "    def _norm(self,x):\n",
    "        # x: [batch,seq_len,hidden_dim]\n",
    "        # pow => mean得rms => +eps => rsqrt\n",
    "        \n",
    "        return x * torch.rsqrt((x**2).mean(dim = -1, keepdim = True) + self.eps)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x.float() 保证计算精度\n",
    "        # type_as(x) 恢复原始数据类型\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight\n",
    "    \n",
    "x = torch.randn(2,3,4)\n",
    "layer = RMSNorm(h=4)\n",
    "\n",
    "out = layer(x)\n",
    "print(out)\n",
    "print(out.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c930a9",
   "metadata": {},
   "source": [
    "# 3.FNN相关\n",
    "- Transformer FNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cec6413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # 实际上就是 MLP\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "             # 激活函数\n",
    "             nn.ReLU(),  \n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "x = torch.randn(2,3,4)\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_embd = 4\n",
    "        self.dropout = 0.1\n",
    "\n",
    "net = FeedForward(Config())\n",
    "out = net(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd7cf4b",
   "metadata": {},
   "source": [
    "- FNN\n",
    "\n",
    "1. dim -> 4*dim- > GeLU -> dim\n",
    "\n",
    "2. 添加了门控、逐元素相乘，act_func替换了 $\\text{SiLU}$ 或 $\\text{SwiGLU}$\n",
    "refer minimind理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c503f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 4*dim), # up\n",
    "            nn.GELU(approximate='tanh'), # activation\n",
    "            nn.Linear(4*dim, dim)\n",
    "            \n",
    "        )       \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out  = self.net(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109f4017",
   "metadata": {},
   "source": [
    "- MoE\n",
    "直接参考chaofa的了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ffcdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 16]) torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# MoE是mask router，token没稀疏化\n",
    "# TKSA是根据router weight做稀疏化token【就是分类器那篇 】\n",
    "\n",
    "# 1.FNN层【Expert】\n",
    "class BasicExpert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 4 * input_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(4 * input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 2.Router【我感觉这个就是后面我TKSA修改的代码】\n",
    "# 主要参考自 mistral MOE 的实现\n",
    "class MOERouter(nn.Module):\n",
    "    def __init__(self, hidden_dim, expert_number, top_k):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_dim, expert_number)  ###！router！ 【TKSA就会模仿这个】\n",
    "        self.expert_number = expert_number\n",
    "        self.top_k = top_k\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # 计算路由logits\n",
    "        # 对于每个token，计算它路由到每个专家的logits\n",
    "        # hidden_states shape is (b * s, hidden_dim) 【！】\n",
    "        router_logits = self.gate(hidden_states)  # shape is (b * s, expert_number)\n",
    "        \n",
    "        # 计算专家经过softmax之后的概率\n",
    "        routing_probs = F.softmax(router_logits, dim=-1, dtype=torch.float)\n",
    "        \n",
    "        # 选择的idxa与weights\n",
    "        router_weights, selected_experts = torch.topk(\n",
    "            routing_probs, self.top_k, dim=-1\n",
    "        )  # shape都是 (b * s, top_k)\n",
    "        \n",
    "        # 选择专家weight重新分配【！】\n",
    "        router_weights = router_weights / router_weights.sum(dim=-1, keepdim=True)\n",
    "        router_weights = router_weights.to(hidden_states.dtype)\n",
    "        \n",
    "        # 生成专家掩码\n",
    "        expert_mask = F.one_hot(\n",
    "            selected_experts,\n",
    "            num_classes=self.expert_number\n",
    "        )  # shape是 (b * s, top_k, expert_number)\n",
    "        expert_mask = expert_mask.permute(2, 1, 0)  # (expert_number, top_k, b * s)\n",
    "        \n",
    "        return router_logits, router_weights, selected_experts, expert_mask\n",
    "\n",
    "\n",
    "class MOEConfig:\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_dim, \n",
    "            expert_number, \n",
    "            top_k, \n",
    "            shared_experts_number=2,\n",
    "        ):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.expert_number = expert_number\n",
    "        self.top_k = top_k\n",
    "        self.shared_experts_number = shared_experts_number\n",
    "\n",
    "class SparseMOE(nn.Module):\n",
    "    # 稀疏 MOE 模型，这里每一个 token 都会过 topk 个专家，得到对应token 的 hidden_embeddings\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "\n",
    "        self.expert_number = config.expert_number\n",
    "        self.top_k = config.top_k\n",
    "\n",
    "        self.experts = nn.ModuleList(\n",
    "            [\n",
    "                BasicExpert(self.hidden_dim, self.hidden_dim) for _ in range(self.expert_number) # 1. 初始化很多FNN层\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.router = MOERouter(self.hidden_dim, self.expert_number, self.top_k)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape is (b, s, hidden_dim)\n",
    "        batch_size, seq_len, hidden_dim = x.size()\n",
    "\n",
    "        # 因为后面是一个个token进行过专家处理，所以batch_size和seq_len要合并\n",
    "        hidden_states = x.view(-1, hidden_dim) # shape is(b * s, hidden_dim)\n",
    "\n",
    "        router_logits, router_weights, selected_experts_indices, expert_mask = self.router(hidden_states) # 得到weight\n",
    "        # 其中 selected_experts_indices shape 是 (b * s, top_k)\n",
    "        # 其中 expert_mask shape 是 (expert_number, top_k, b * s)\n",
    "        \n",
    "        final_hidden_states = torch.zeros(\n",
    "            (batch_size * seq_len, hidden_dim),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "\n",
    "        for expert_idx in range(self.expert_number):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            # expert_mask[expert_idx] shape 是 (top_k, b * s)\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx]) \n",
    "            # idx 和 top_x 都是一维 tensor\n",
    "            # idx 的值是 0 或 1, 表示这个 token 是作为当前专家的 top1 还是 top2\n",
    "            # top_x 的值是 token 在 batch*seq_len 中的位置索引\n",
    "            # 例如对于 batch_size=2, seq_len=4 的输入:\n",
    "            # top_x 的值范围是 0-7, 表示在展平后的 8 个 token 中的位置\n",
    "            # idx 的值是 0/1, 表示这个 token 把当前专家作为其 top1/top2 专家\n",
    "\n",
    "            # hidden_states 的 shape 是 (b * s, hidden_dim)\n",
    "            # 需要取到 top_x 对应的 hidden_states\n",
    "            current_state = hidden_states.unsqueeze(\n",
    "                0\n",
    "            )[:, top_x, :].reshape(-1, hidden_dim) # （selected_token_number, hidden_dim）    ####！！！还没看懂\n",
    "\n",
    "            # router_weight 的 shape 是 (b * s, top_k)\n",
    "            current_hidden_states = expert_layer(\n",
    "                current_state\n",
    "            ) * router_weights[top_x, idx].unsqueeze(-1)  # （selected_token_number, 1） 这里有广播\n",
    "\n",
    "            # 把当前专家的输出加到 final_hidden_states 中\n",
    "            # 方式1 的写法性能更好，并且方式1容易出现\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n",
    "            # 方式2\n",
    "            # final_hidden_states[top_x] += current_hidden_states.to(hidden_states.dtype)\n",
    "            # 方式2 的写法性能更差，并且方式2容易出现错误，+= 操作在处理重复索引时需要多次读写内存，可能会导致竞争条件\n",
    "\n",
    "        # 把 final_hidden_states 还原到原来的 shape\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "        return final_hidden_states, router_logits # shape 是 (b * s, expert_number)\n",
    "\n",
    "\n",
    "def test_token_level_moe():\n",
    "    x = torch.rand(2, 4, 16)\n",
    "    config = MOEConfig(16, 2, 2)\n",
    "    token_level_moe = SparseMOE(config)\n",
    "    out = token_level_moe(x)\n",
    "    print(out[0].shape, out[1].shape)\n",
    "\n",
    "\n",
    "test_token_level_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c943e2ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
